{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31eac6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:52:42.988692Z",
     "start_time": "2024-07-16T21:52:39.162226Z"
    }
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "from necessary_functions import get_numtadb_training_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a0430",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7678a3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:54:44.791804Z",
     "start_time": "2024-07-16T09:53:57.264323Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loading is Complete\n"
     ]
    }
   ],
   "source": [
    "# the first layer of AlexNet intakes a 227x227 image.\n",
    "# causes memory error, so just let's load these as 32x32 images and later we will add a resize layer to our model\n",
    "\n",
    "images, labels = get_numtadb_training_data(\n",
    "    dataset_directory=\"NumtaDB_Bengali Handwritten Digits/\", img_resize_size=(32, 32), img_fmt='color'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92bb39d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:54:44.807268Z",
     "start_time": "2024-07-16T09:54:44.795720Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72045 72045\n"
     ]
    }
   ],
   "source": [
    "# total number of images\n",
    "print(len(images), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabd0b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:54:44.838318Z",
     "start_time": "2024-07-16T09:54:44.813344Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "# generate indices for masking and shuffling them\n",
    "index = list(range(len(labels)))\n",
    "np.random.shuffle(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44d4f52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:54:45.258759Z",
     "start_time": "2024-07-16T09:54:44.843262Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# now we will take 80% of the data as train set and 20% as test set\n",
    "train_amount = int(len(labels)*0.8)\n",
    "X_train = images[index[:train_amount]]\n",
    "X_test = images[index[train_amount:]]\n",
    "y_train = labels[index[:train_amount]]\n",
    "y_test = labels[index[train_amount:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7967650d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:54:45.445154Z",
     "start_time": "2024-07-16T09:54:45.263637Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding label is 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check 1\n",
    "plt.imshow(X_train[57], 'gray')\n",
    "print(f\"The corresponding label is {y_train[57]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ec1173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:54:45.460277Z",
     "start_time": "2024-07-16T09:54:45.450219Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data are 57636 and 57636\n",
      "Size of testing data are 14409 and 14409\n"
     ]
    }
   ],
   "source": [
    "# sanity check 2\n",
    "print(f\"Size of training data are {len(X_train)} and {len(y_train)}\")\n",
    "print(f\"Size of testing data are {len(X_test)} and {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cbf7c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# AlexNet Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251b459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T07:28:30.215945Z",
     "start_time": "2024-07-16T07:28:30.184440Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "![AlexNet Architecture](AlexNet-1-1430805382.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e34b49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The above figure shows the AlexNet Architecture and we are going to implement it using tensorflow. Aside from the layers shown in the figure the authors mentions in the paper that the output from all the layers are followed by 'relu' activation function and there are local response normalization layers following the first two 'relu'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "652a44d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:54:45.476339Z",
     "start_time": "2024-07-16T09:54:45.464403Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# using functional api\n",
    "def alexnet(input_shape):\n",
    "    input_img = tf.keras.layers.Input(input_shape)\n",
    "    img_resized = tf.keras.layers.Resizing(227,227)(input_img) # resizing before passing to the first convolutional layer\n",
    "    \n",
    "    C1 = tf.keras.layers.Conv2D(filters = 96, kernel_size = 11, strides = 4, padding = 'valid')(img_resized)\n",
    "    A1 = tf.keras.layers.Activation('relu')(C1)\n",
    "    LRN1 = tf.keras.layers.Lambda(tf.nn.local_response_normalization)(A1)\n",
    "    P1 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(LRN1)\n",
    "    \n",
    "    C2 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 5, strides = 1, padding = 'same')(P1)\n",
    "    A2 = tf.keras.layers.Activation('relu')(C2)\n",
    "    LRN2 = tf.keras.layers.Lambda(tf.nn.local_response_normalization)(A2)\n",
    "    P2 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(LRN2)\n",
    "    \n",
    "    C3 = tf.keras.layers.Conv2D(filters = 384, kernel_size = 3, strides = 1, padding = 'same')(P2)\n",
    "    A3 = tf.keras.layers.Activation('relu')(C3)\n",
    "    C4 = tf.keras.layers.Conv2D(filters = 384, kernel_size = 3, strides = 1, padding = 'same')(A3)\n",
    "    A4 = tf.keras.layers.Activation('relu')(C4)\n",
    "    C5 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(A4)\n",
    "    A5 = tf.keras.layers.Activation('relu')(C5)\n",
    "    \n",
    "    P3 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(A5)\n",
    "    \n",
    "    P3_flattened = tf.keras.layers.Flatten()(P3)\n",
    "    FC1 = tf.keras.layers.Dense(4096, activation='relu')(P3_flattened)\n",
    "    FC2 = tf.keras.layers.Dense(4096, activation='relu')(FC1)\n",
    "    output = tf.keras.layers.Dense(10, activation='softmax')(FC2) # 10 neurons as there are only 10 classes\n",
    "    \n",
    "    AlexNet = tf.keras.Model(inputs=input_img, outputs=output)\n",
    "    return AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e3e1908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:54:47.076676Z",
     "start_time": "2024-07-16T09:54:45.480339Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 227, 227, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 55, 55, 96)        0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 55, 55, 96)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 27, 27, 256)       0         \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 27, 27, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 13, 13, 384)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 13, 384)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 13, 13, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,322,314\n",
      "Trainable params: 58,322,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AlexNet = alexnet(input_shape=(32,32,3))\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad2f87",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f5fc3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:41:18.862271Z",
     "start_time": "2024-07-16T09:54:47.079542Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "721/721 [==============================] - 1371s 2s/step - loss: 1.9452 - accuracy: 0.2648 - val_loss: 1.8175 - val_accuracy: 0.2933\n",
      "Epoch 2/10\n",
      "721/721 [==============================] - 1365s 2s/step - loss: 1.8181 - accuracy: 0.2933 - val_loss: 1.8124 - val_accuracy: 0.2936\n",
      "Epoch 3/10\n",
      "721/721 [==============================] - 1358s 2s/step - loss: 1.8065 - accuracy: 0.2962 - val_loss: 1.8140 - val_accuracy: 0.2916\n",
      "Epoch 4/10\n",
      "721/721 [==============================] - 1351s 2s/step - loss: 1.7950 - accuracy: 0.3008 - val_loss: 1.8056 - val_accuracy: 0.2946\n",
      "Epoch 5/10\n",
      "721/721 [==============================] - 1346s 2s/step - loss: 1.7907 - accuracy: 0.3021 - val_loss: 1.8102 - val_accuracy: 0.2963\n",
      "Epoch 6/10\n",
      "721/721 [==============================] - 1352s 2s/step - loss: 1.7898 - accuracy: 0.3034 - val_loss: 1.8137 - val_accuracy: 0.2938\n",
      "Epoch 7/10\n",
      "721/721 [==============================] - 1363s 2s/step - loss: 1.7899 - accuracy: 0.3034 - val_loss: 1.7980 - val_accuracy: 0.3018\n",
      "Epoch 8/10\n",
      "721/721 [==============================] - 1350s 2s/step - loss: 1.7847 - accuracy: 0.3031 - val_loss: 1.8165 - val_accuracy: 0.2961\n",
      "Epoch 9/10\n",
      "721/721 [==============================] - 1349s 2s/step - loss: 1.7822 - accuracy: 0.3052 - val_loss: 1.8143 - val_accuracy: 0.2988\n",
      "Epoch 10/10\n",
      "721/721 [==============================] - 1385s 2s/step - loss: 1.7849 - accuracy: 0.3076 - val_loss: 1.8063 - val_accuracy: 0.3000\n"
     ]
    }
   ],
   "source": [
    "AlexNet.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    history = AlexNet.fit(\n",
    "        X_train, y_train, validation_split=0.2, epochs=10, batch_size=64,\n",
    "    )  # and 20% of training data is used as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65fbf192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:41:37.677217Z",
     "start_time": "2024-07-16T13:41:20.130640Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451/451 [==============================] - 17s 32ms/step - loss: 1.8067 - accuracy: 0.3031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.806747317314148, 0.3030744791030884]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexNet.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5a9da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Checking performance on mnist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fb710",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "lets check with a standard dataset namely mnist dataset and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c353c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T14:46:09.952806Z",
     "start_time": "2024-07-16T14:46:09.945802Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# using functional api\n",
    "def alexnet(input_shape):\n",
    "    input_img = tf.keras.layers.Input(input_shape)\n",
    "    img_resized = tf.keras.layers.Resizing(227,227)(input_img) # resizing before passing to the first convolutional layer\n",
    "    \n",
    "    C1 = tf.keras.layers.Conv2D(filters = 96, kernel_size = 11, strides = 4, padding = 'valid')(img_resized)\n",
    "    A1 = tf.keras.layers.Activation('relu')(C1)\n",
    "    LRN1 = tf.keras.layers.Lambda(tf.nn.local_response_normalization)(A1)\n",
    "    P1 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(LRN1)\n",
    "    \n",
    "    C2 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 5, strides = 1, padding = 'same')(P1)\n",
    "    A2 = tf.keras.layers.Activation('relu')(C2)\n",
    "    LRN2 = tf.keras.layers.Lambda(tf.nn.local_response_normalization)(A2)\n",
    "    P2 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(LRN2)\n",
    "    \n",
    "    C3 = tf.keras.layers.Conv2D(filters = 384, kernel_size = 3, strides = 1, padding = 'same')(P2)\n",
    "    A3 = tf.keras.layers.Activation('relu')(C3)\n",
    "    C4 = tf.keras.layers.Conv2D(filters = 384, kernel_size = 3, strides = 1, padding = 'same')(A3)\n",
    "    A4 = tf.keras.layers.Activation('relu')(C4)\n",
    "    C5 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(A4)\n",
    "    A5 = tf.keras.layers.Activation('relu')(C5)\n",
    "    \n",
    "    P3 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(A5)\n",
    "    \n",
    "    P3_flattened = tf.keras.layers.Flatten()(P3)\n",
    "    FC1 = tf.keras.layers.Dense(4096, activation='relu')(P3_flattened)\n",
    "    FC2 = tf.keras.layers.Dense(4096, activation='relu')(FC1)\n",
    "    output = tf.keras.layers.Dense(10, activation='softmax')(FC2) # 10 neurons as there are only 10 classes\n",
    "    \n",
    "    AlexNet = tf.keras.Model(inputs=input_img, outputs=output)\n",
    "    return AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85956fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T14:46:11.149138Z",
     "start_time": "2024-07-16T14:46:10.376525Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 227, 227, 1)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        11712     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 55, 55, 96)        0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 55, 55, 96)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 27, 27, 256)       0         \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 27, 27, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 13, 13, 384)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 13, 384)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 13, 13, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,299,082\n",
      "Trainable params: 58,299,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AlexNet = alexnet(input_shape=(28,28,1))\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb07d45e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T18:38:46.905720Z",
     "start_time": "2024-07-16T14:46:15.497802Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 1494s 2s/step - loss: 0.2779 - accuracy: 0.9240 - val_loss: 0.0725 - val_accuracy: 0.9796\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 1472s 2s/step - loss: 0.0650 - accuracy: 0.9802 - val_loss: 0.0616 - val_accuracy: 0.9831\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 1541s 2s/step - loss: 0.0585 - accuracy: 0.9833 - val_loss: 0.0578 - val_accuracy: 0.9834\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 1403s 2s/step - loss: 0.0453 - accuracy: 0.9868 - val_loss: 0.0580 - val_accuracy: 0.9877\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 1355s 2s/step - loss: 0.0382 - accuracy: 0.9890 - val_loss: 0.0428 - val_accuracy: 0.9876\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 1337s 2s/step - loss: 0.0380 - accuracy: 0.9891 - val_loss: 0.0523 - val_accuracy: 0.9865\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 1342s 2s/step - loss: 0.0297 - accuracy: 0.9913 - val_loss: 0.0392 - val_accuracy: 0.9890\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 1340s 2s/step - loss: 0.0321 - accuracy: 0.9908 - val_loss: 0.0537 - val_accuracy: 0.9865\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 1334s 2s/step - loss: 0.0298 - accuracy: 0.9912 - val_loss: 0.0772 - val_accuracy: 0.9810\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 1333s 2s/step - loss: 0.0238 - accuracy: 0.9933 - val_loss: 0.0632 - val_accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "AlexNet.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    history2 = AlexNet.fit(\n",
    "        X_train, y_train, validation_split=0.2, epochs=10, batch_size=64,\n",
    "    )  # and 20% of training data is used as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94d109d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:09:47.576641Z",
     "start_time": "2024-07-16T21:09:33.316925Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 14s 30ms/step - loss: 0.0489 - accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04892425611615181, 0.9883000254631042]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexNet.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdd243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "433bc776",
   "metadata": {},
   "source": [
    "The training is very slow on cpu but as gpu memory is also less (4GB) so training for higher number of epochs is not possible. Also it seems that the model works better for mnist dataset than the bengali handwritten digit dataset. it could be due to grayscaling of image. Let's check for this hypothesis next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefeeac",
   "metadata": {},
   "source": [
    "# Checking performance on grayscaled bengali handwritten digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e3e86",
   "metadata": {},
   "source": [
    "Also lets use Batchnormalization instead of LRN. as mentioned in https://medium.com/swlh/alexnet-with-tensorflow-46f366559ce8 LRN slows down the implementation and does not significantly contribute towards accuracy. Also we will modify the main base architecture by modifying the number of input channels to each layer. This will be done to reduce the number of parameters that needs to be learned. Current implementation has about 58.32M parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70cf967",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:53:24.128463Z",
     "start_time": "2024-07-16T21:52:42.988692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loading is Complete\n"
     ]
    }
   ],
   "source": [
    "# the first layer of AlexNet intakes a 227x227 image.\n",
    "# causes memory error, so just let's load these as 32x32 images and later we will add a resize layer to our model\n",
    "\n",
    "images, labels = get_numtadb_training_data(\n",
    "    dataset_directory=\"NumtaDB_Bengali Handwritten Digits/\", img_resize_size=(28, 28), img_fmt='grayscale'\n",
    ")\n",
    "\n",
    "np.random.seed(7)\n",
    "# generate indices for masking and shuffling them\n",
    "index = list(range(len(labels)))\n",
    "np.random.shuffle(index)\n",
    "\n",
    "# now we will take 80% of the data as train set and 20% as test set\n",
    "train_amount = int(len(labels)*0.8)\n",
    "X_train = images[index[:train_amount]]\n",
    "X_test = images[index[train_amount:]]\n",
    "y_train = labels[index[:train_amount]]\n",
    "y_test = labels[index[train_amount:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6643aaa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:53:24.139986Z",
     "start_time": "2024-07-16T21:53:24.130469Z"
    }
   },
   "outputs": [],
   "source": [
    "# using functional api\n",
    "def alexnet(input_shape):\n",
    "    input_img = tf.keras.layers.Input(input_shape)\n",
    "    img_resized = tf.keras.layers.Resizing(227,227)(input_img) # resizing before passing to the first convolutional layer\n",
    "    \n",
    "    C1 = tf.keras.layers.Conv2D(filters = 96, kernel_size = 11, strides = 4, padding = 'valid')(img_resized)\n",
    "    A1 = tf.keras.layers.Activation('relu')(C1)\n",
    "    LRN1 = tf.keras.layers.Lambda(tf.nn.local_response_normalization)(A1)\n",
    "    P1 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(LRN1)\n",
    "    \n",
    "    C2 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 5, strides = 1, padding = 'same')(P1)\n",
    "    A2 = tf.keras.layers.Activation('relu')(C2)\n",
    "    LRN2 = tf.keras.layers.Lambda(tf.nn.local_response_normalization)(A2)\n",
    "    P2 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(LRN2)\n",
    "    \n",
    "    C3 = tf.keras.layers.Conv2D(filters = 384, kernel_size = 3, strides = 1, padding = 'same')(P2)\n",
    "    A3 = tf.keras.layers.Activation('relu')(C3)\n",
    "    C4 = tf.keras.layers.Conv2D(filters = 384, kernel_size = 3, strides = 1, padding = 'same')(A3)\n",
    "    A4 = tf.keras.layers.Activation('relu')(C4)\n",
    "    C5 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(A4)\n",
    "    A5 = tf.keras.layers.Activation('relu')(C5)\n",
    "    \n",
    "    P3 = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(A5)\n",
    "    \n",
    "    P3_flattened = tf.keras.layers.Flatten()(P3)\n",
    "    FC1 = tf.keras.layers.Dense(4096, activation='relu')(P3_flattened)\n",
    "    FC2 = tf.keras.layers.Dense(4096, activation='relu')(FC1)\n",
    "    output = tf.keras.layers.Dense(10, activation='softmax')(FC2) # 10 neurons as there are only 10 classes\n",
    "    \n",
    "    AlexNet = tf.keras.Model(inputs=input_img, outputs=output)\n",
    "    return AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12735a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:53:26.368781Z",
     "start_time": "2024-07-16T21:53:24.142018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 227, 227, 1)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        11712     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 55, 55, 96)        0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 55, 55, 96)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 27, 27, 256)       0         \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 27, 27, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 13, 13, 384)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 13, 384)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 13, 13, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,299,082\n",
      "Trainable params: 58,299,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AlexNet = alexnet(input_shape=(28,28,1))\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa164b86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T22:12:27.765164Z",
     "start_time": "2024-07-16T21:53:26.369828Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "689/721 [===========================>..] - ETA: 52s - loss: 1.9283 - accuracy: 0.2575"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m AlexNet\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/CPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     history3 \u001b[38;5;241m=\u001b[39m \u001b[43mAlexNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlai\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AlexNet.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    history3 = AlexNet.fit(\n",
    "        X_train, y_train, validation_split=0.2, epochs=10, batch_size=64,\n",
    "    )  # and 20% of training data is used as validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ad5ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T22:12:27.770723Z",
     "start_time": "2024-07-16T22:12:27.770193Z"
    }
   },
   "source": [
    "Doesn't seem to improve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlai_specialization",
   "language": "python",
   "name": "dlai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
